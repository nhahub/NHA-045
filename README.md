<div align="center">

<img src="logo2.png" alt="SEMSOL Logo" width="1500" style="border-radius: 50%;"/>

#  SEMSOL - Student Engagement Monitoring System

<div align="center">

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)
![Streamlit](https://img.shields.io/badge/Streamlit-1.28+-ff4b4b.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)

**Real-time Multi-Modal Student Engagement Detection using Computer Vision & Deep Learning**

[Demo](#-demo) â€¢ [Features](#-features) â€¢ [Installation](#-installation) â€¢ [Usage](#-usage) â€¢ [Documentation](#-documentation)

---

##  Demo

<div align="center">

### ğŸ“¹ Watch SEMSOL in Action

[![SEMSOL Demo Video](https://img.youtube.com/vi/_XyUh5nAyrw/maxresdefault.jpg)](https://www.youtube.com/watch?v=_XyUh5nAyrw)

**Click to watch:** Real-time engagement monitoring with live analytics and session reports

</div>

---

## ğŸ¯ Overview

**SEMSOL** (Student Engagement Monitoring System for Online Learning) is an intelligent system that monitors and analyzes student engagement in real-time using advanced computer vision and machine learning techniques. The system combines **gaze estimation**, **blink detection**, and **deep learning classification** to provide comprehensive engagement analytics.

### ğŸŒŸ Why SEMSOL?

-  **Educational Insights**: Help educators understand student attention patterns
-  **Research Tool**: Collect quantitative engagement data for academic research
-  **Productivity Tracking**: Monitor focus and concentration during work/study
-  **Accessibility**: Analyze visual attention patterns for inclusive design
-  **Data-Driven Decisions**: Make informed improvements based on engagement metrics

---

## âœ¨ Features

### ğŸ¥ Real-Time Monitoring

- ** Blink Detection**: Tracks eye blinks to detect drowsiness, stress, and distraction
- ** Gaze Estimation**: Monitors head orientation (pitch/yaw) to track attention direction
- ** ML Classification**: AI-powered engagement level prediction (4 levels)
- ** Multi-Source Support**: Works with webcam or pre-recorded video files

### ğŸ“Š Comprehensive Analytics

- **Interactive Dashboards**: Real-time metrics and visualizations
- **Engagement Timeline**: Track engagement changes throughout the session
- **Gaze Heatmaps**: Visualize where attention was focused
- **Blink Analysis**: Monitor eye activity and fatigue indicators
- **Statistical Summaries**: Detailed breakdowns of all metrics

### ğŸ’¾ Export & Reporting

- **CSV Export**: Raw data for further analysis
- **JSON Reports**: Structured summaries with statistics
- **Interactive HTML Charts**: Standalone visualizations (Plotly)
- **Session Summaries**: Comprehensive engagement breakdowns

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- Webcam (for live monitoring) or video files
- CUDA-capable GPU (optional, for faster inference)

### Installation

```bash
# Clone the repository
git clone https://github.com/Mario-Gamal-Sobhy/SEMSOL.git
cd SEMSOL
git checkout V1.2

# Install dependencies
pip install -r requirements.txt
```

### Download Model Weights

Place the following model files in the `weights/` directory:

| Model | Size | Description | Download |
|-------|------|-------------|----------|
| `resnet34.pt` | ~85 MB | Gaze estimation model | [Link](#) |
| `engagement_classifier.pkl` | ~875 KB | Engagement classifier | [Link](#) |

> **Note**: Model files are not included in the repository due to size constraints. Download them separately.

### Run the Application

```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

---

## ğŸ“– Usage

### 1ï¸âƒ£ Configure Settings

- **Video Source**: Choose webcam (0, 1, 2) or upload video file
- **Detection Options**: Enable/disable blink detection
- **ML Classifier**: Toggle trained model vs rule-based classification
- **Visual Settings**: Customize bounding boxes and gaze arrows

### 2ï¸âƒ£ Start Monitoring

1. Click **"ğŸš€ Start Monitoring"** in the sidebar
2. Wait for camera initialization (2-3 seconds)
3. Blink detector calibrates automatically (50 frames)
4. Real-time metrics appear on the right panel

### 3ï¸âƒ£ View Analytics

- **For Videos**: Statistics auto-generate when video ends
- **For Webcam**: Click **"â¹ Stop Monitoring"** to view analytics
- Explore interactive charts and download reports

### 4ï¸âƒ£ Export Data

Download your session data in multiple formats:
- ğŸ“„ **CSV**: Full dataset with timestamps
- ğŸ“‹ **JSON**: Summary statistics and metrics
- ğŸ“Š **HTML**: Interactive charts (open in browser)

---

## ğŸ“ Engagement Levels

The system classifies engagement into 4 distinct levels:

| Level | Emoji | Description | Indicators |
|-------|-------|-------------|------------|
| **Highly Engaged** | ğŸŸ¢ | Fully attentive and focused | Looking at screen, normal blink rate, stable gaze |
| **Engaged** | ğŸŸ¡ | Generally attentive | Mostly on-task with minor distractions |
| **Partially Engaged** | ğŸŸ  | Distracted or wandering | Irregular gaze, looking away frequently |
| **Disengaged** | ğŸ”´ | Not paying attention | Looking away, drowsy, or distracted |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Input Stream                          â”‚
â”‚                    (Webcam / Video File)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Face Detection                            â”‚
â”‚                   (RetinaFace / UniFace)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                       â”‚
           â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Gaze Estimation    â”‚  â”‚  Blink Detection     â”‚
â”‚   (ResNet-34)        â”‚  â”‚  (EAR Algorithm)     â”‚
â”‚   â€¢ Pitch angle      â”‚  â”‚  â€¢ Blink rate        â”‚
â”‚   â€¢ Yaw angle        â”‚  â”‚  â€¢ EAR values        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                         â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Feature Extraction    â”‚
           â”‚  â€¢ Aggregated stats    â”‚
           â”‚  â€¢ Temporal patterns   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  ML Classification     â”‚
           â”‚  (Random Forest / CNN) â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Engagement Level      â”‚
           â”‚  (1-4 + Confidence)    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Metrics Explained

### Gaze Metrics
- **Pitch**: Vertical head angle (-90Â° to +90Â°)
- **Yaw**: Horizontal head angle (-90Â° to +90Â°)
- **Looking at Screen**: |pitch| < 12Â° AND |yaw| < 15Â°

### Blink Metrics
- **EAR** (Eye Aspect Ratio): Measure of eye openness (0.0 - 1.0)
  - Typical range: 0.25 - 0.35
  - Blink detected: < 0.20
- **Blink Rate**: Blinks per second (calculated over 10s window)
  - Normal: 0.15 - 0.25 bps (9-15 bpm)
  - Drowsy: < 0.10 bps
  - Stressed: > 0.35 bps

### Engagement Score
- Overall score: 0-100 (weighted average)
- Formula: `(HighlyÃ—100 + EngagedÃ—70 + PartialÃ—40 + DisengagedÃ—10) / total_frames`

---

## ğŸ› ï¸ Advanced Configuration

### Camera Troubleshooting

If camera doesn't work:

1. **Test Camera**: Click "ğŸ§ª Test Camera" in sidebar
2. **Close competing apps**: Zoom, Teams, Skype, OBS
3. **Try different indices**: Switch between Webcam (0), (1), (2)
4. **Check permissions**: Allow camera access in system settings

### Model Configuration

Edit `config.py` to customize:

```python
data_config = {
    "gaze360": {
        "bins": 90,
        "binwidth": 4,
        "angle": 180
    }
}
```

### Sensitivity Tuning

- **EAR Sensitivity**: Lower = more sensitive blink detection (default: 0.80)
- **Target FPS**: Higher = faster processing, more CPU usage (default: 15)
- **Gaze Model**: Choose `resnet34` (more accurate) or `resnet18` (faster)

---

## ğŸ“ Project Structure

```
SEMSOL/
â”œâ”€â”€ app.py                          # Main Streamlit application
â”œâ”€â”€ config.py                       # Configuration settings
â”œâ”€â”€ uniface.py                      # Face detection module
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ README.md                       # This file
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ blink_detector.py          # Blink detection logic
â”‚   â”œâ”€â”€ helpers.py                 # Utility functions
â”‚   â”œâ”€â”€ ml_engagement_classifier.py # ML classifier training
â”‚   â””â”€â”€ save_model_example.py      # Model export utilities
â”‚
â”œâ”€â”€ weights/
â”‚   â”œâ”€â”€ resnet34.pt                # Gaze estimation weights
â”‚   â””â”€â”€ engagement_classifier.pkl  # Trained classifier
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ in_video.mp4               # Sample test video
â”‚
â””â”€â”€ docs/
    â””â”€â”€ API.md                      # API documentation
```

---

## ğŸ”¬ Research & Citations --

If you use SEMSOL in your research, please cite:

```bibtex
@software{semsol2024,
  author = {Mario Gamal Sobhy},
  title = {SEMSOL: Student Engagement Monitoring System for Online Learning},
  year = {2024},
  url = {https://github.com/Mario-Gamal-Sobhy/SEMSOL}
}
```

### Related Work

- **Gaze Estimation**: Based on Gaze360 dataset and ResNet architectures
- **Blink Detection**: Uses EAR (Eye Aspect Ratio) method from SoukupovÃ¡ & ÄŒech (2016)
- **Face Detection**: RetinaFace for robust face localization

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these guidelines:

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/AmazingFeature`)
3. **Commit** your changes (`git commit -m 'Add AmazingFeature'`)
4. **Push** to the branch (`git push origin feature/AmazingFeature`)
5. **Open** a Pull Request

### Development Setup

```bash
# Clone your fork
git clone https://github.com/YOUR_USERNAME/SEMSOL.git
cd SEMSOL

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dev dependencies
pip install -r requirements.txt
pip install black flake8 pytest
```

---

## ğŸ› Known Issues & Limitations

- **Camera Compatibility**: Some laptops require specific backends (DSHOW, V4L2)
- **Model Size**: Deep learning models require ~200MB disk space
- **GPU Required**: Real-time processing works best with CUDA GPU
- **Lighting Conditions**: Performance degrades in very low light
- **Multiple Faces**: Currently optimized for single-person monitoring

---

## ğŸ“… Roadmap

- [ ] **Multi-person tracking**: Support multiple students simultaneously
- [ ] **Audio analysis**: Integrate speech engagement detection
- [ ] **Mobile support**: Android/iOS app versions
- [ ] **Cloud deployment**: Scalable infrastructure for classrooms
- [ ] **LMS integration**: Canvas, Moodle, Blackboard plugins
- [ ] **Privacy features**: On-device processing, no data upload
- [ ] **Real-time alerts**: Notify educators of disengagement

---

## ğŸ“œ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**Mario Gamal Sobhy**

- GitHub: [@Mario-Gamal-Sobhy](https://github.com/Mario-Gamal-Sobhy)
- Email: [your.email@example.com](mailto:your.email@example.com)

---

## ğŸ™ Acknowledgments

- **Gaze360 Dataset**: For gaze estimation training data
- **RetinaFace**: For robust face detection
- **Streamlit**: For the amazing web framework
- **PyTorch Community**: For deep learning tools
- **Open Source Contributors**: For libraries and inspiration

---

## ğŸ“ Support

Having issues? Here's how to get help:

1. **Check Documentation**: Read the [Usage](#-usage) section
2. **Search Issues**: Look through [existing issues](https://github.com/Mario-Gamal-Sobhy/SEMSOL/issues)
3. **Create Issue**: Open a [new issue](https://github.com/Mario-Gamal-Sobhy/SEMSOL/issues/new) with details
4. **Discussions**: Join our [GitHub Discussions](https://github.com/Mario-Gamal-Sobhy/SEMSOL/discussions)

---

<div align="center">

**â­ If you find SEMSOL helpful, please consider giving it a star!**

Made with â¤ï¸ by [Mario Gamal Sobhy](https://github.com/Mario-Gamal-Sobhy)
                 [Maximos Naseef Bassiet](https://github.com/maxemosnassef-dotcom)


[â¬† Back to Top](#-semsol---student-engagement-monitoring-system)

</div>
